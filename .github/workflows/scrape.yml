name: Daily Scraper

on:
  schedule:
    - cron: '0 15 * * *'   # Runs daily at 09:00 UTC
  workflow_dispatch:       # Allows manual triggering from GitHub UI

jobs:
  run-r-script:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libcurl4-openssl-dev libxml2-dev libssl-dev


      - name: Install base dependencies
        run: Rscript -e 'install.packages(c("dplyr","tidyverse", "httr", "jsonlite", "assertthat", "fs", "lubridate", "xml2", "stringr", "readr", "rvest", "tictoc", "chromote","R.utils"), repos = "https://cloud.r-project.org")'

      - name: Install rvest
        run: Rscript -e 'install.packages("rvest", repos = "https://cloud.r-project.org")'

      - name: Check installed packages
        run: Rscript -e 'print(installed.packages()[, "Package"])'

      - name: Run daily scraper
        run: Rscript code/daily_scraper.R

      - name: Commit and push new data
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/new/*.csv
          git add data/model/*.rds
          git commit -m "Update scraped data [$(date +'%Y-%m-%d')]"
          git push
        env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
